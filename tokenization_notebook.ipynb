{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19e2a60",
   "metadata": {},
   "source": [
    "Note, ord takes in a string and reurns unicode (.encode('utf-8)), chr does the oposite, it takes in unicode and returns a strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c109a",
   "metadata": {},
   "source": [
    "Spot the most common pair, and replace it with a new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2d3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(encoding: list):\n",
    "    counts = {}\n",
    "    for i, j in zip(encoding, encoding[1:]):\n",
    "        counts[(i,j)] = counts.get((i,j), 0) + 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d3340",
   "metadata": {},
   "source": [
    "I need to replace the most common token with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4c1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(encoding:list, pair:tuple, value:int):\n",
    "    i=0\n",
    "    new_encoding = []\n",
    "    while i < len(encoding):\n",
    "        if (i<len(encoding) - 1) and (encoding[i]==pair[0]) and (encoding[i+1] == pair[1]):\n",
    "            new_encoding.append(value)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_encoding.append(encoding[i])\n",
    "            i += 1\n",
    "\n",
    "    return new_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60dd27",
   "metadata": {},
   "source": [
    "Now we have to define how much do we want to shrink the original encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb670eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n"
     ]
    }
   ],
   "source": [
    "text = \"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "\n",
    "vocab = {x:bytes([x]) for x in range(256)} # When you are dealing with bytes and loops make sure to include the [] in the bytes([x]) otherwise you will get a series of byte concatenated.\n",
    "                                           # Not separate bytes\n",
    "\n",
    "\n",
    "# text = 'hello world. I am Myron. A 26 year old student who is studying quantum science and technology at TUM. Although quantum science is an interesting topic I would actually prefer to focus on interpretability of neural networks. I think that is is a fascinated and way too under explored topic. Do you think that I stand a chance to do a PhD on that?'\n",
    "tokens = list(text.encode('utf-8'))\n",
    "\n",
    "text = b\"\".join(vocab[x] for x in tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7299e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 256, 5, 256]\n"
     ]
    }
   ],
   "source": [
    "new_list = merge([10,11,12,99,99,5,99,99], (99,99), 256)\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a405a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    return b\"\".join(vocab[x] for x in tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def encode(text):\n",
    "    return list(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d011e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {}\n",
    "for i in range(1,6):\n",
    "    counts = get_counts(encoding=tokens)\n",
    "    top_pair = max(counts, key=counts.get)\n",
    "    merges[255+i] = top_pair\n",
    "    new_tokens = merge(encoding=tokens, pair=top_pair, value=255+i)\n",
    "    tokens = new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc704493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n"
     ]
    }
   ],
   "source": [
    "while any(t > 255 for t in tokens):\n",
    "    new_tokens = []\n",
    "    key = max(tokens)\n",
    "    for x in tokens:\n",
    "        if x!=key:\n",
    "            new_tokens.append(x)\n",
    "        else:\n",
    "            new_tokens.append(merges[key][0])\n",
    "            new_tokens.append(merges[key][1])\n",
    "    \n",
    "    tokens = new_tokens\n",
    "text = b\"\".join(vocab[x] for x in tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bd853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4T",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
